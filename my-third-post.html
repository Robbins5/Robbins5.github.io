<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Research Tools Are Influencing Decision-Making Without Formal Oversight - Applied AI Observer</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <header>
        <h1>Applied AI Observer</h1>
        <p class="tagline">Research and Coding Assistants in the Enterprise</p>
        <nav>
            <a href="index.html">Home</a> |
            <a href="about.html">About</a>
        </nav>
    </header>

    <main>
        <article>
            <h2>AI Research Tools Are Influencing Decision-Making Without Formal Oversight</h2>
            <p class="date">January 25, 2026</p>

            <p>
                Many organizations have focused their AI governance efforts on approving specific tools, often those
                embedded within enterprise platforms, while restricting others. While this approach addresses vendor
                risk, it does not fully reflect how work is performed in practice. Employees frequently supplement
                approved tools with external AI platforms on personal devices to solve operational or analytical
                challenges more efficiently.
            </p>

            <p>
                In asset-intensive industries, this behavior introduces risks related to proprietary pricing models,
                supplier data, customer information, and operational strategy. Industry guidance from the National
                Association of Electrical Distributors and broader industrial distribution research shows that
                competitive advantage increasingly depends on data and analytics capabilities. Uncontrolled AI use
                can quietly expose this information.
            </p>

            <p>
                This challenge aligns with Institute of Internal Auditors Standard 2110, which focuses on governance,
                and IIA Standard 2100, which emphasizes risk management. COSO similarly recognizes that effective
                internal control depends on both formal policies and human behavior.
            </p>

            <p>
                The implication is that organizations need more mature AI governance structures, including advisory
                boards, clear accountability models, and targeted training tailored to industry-specific risks.
                Governance must address not only which tools are approved, but how employees actually use AI in
                day-to-day work.
            </p>

            <p>
                For internal audit, this shift reinforces the need to evolve from a compliance-focused role to an
                advisory one. Audit teams that invest in AI literacy, understand operational workflows, and engage
                early in governance discussions will be better positioned to help organizations manage AI risk
                proactively rather than reactively.
            </p>
        </article>
    </main>

    <footer>
        <p>&copy; 2026 Nicole Heflin</p>
    </footer>
</body>
</html>

